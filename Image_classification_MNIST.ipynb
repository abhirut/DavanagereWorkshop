{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saneem/anaconda/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "from __future__ import print_function\n",
    "\n",
    "# importing deep learning library\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "def load_images(filename):\n",
    "    f = gzip.open(filename,'r')\n",
    "    image_size = 28\n",
    "    \n",
    "    f.read(16)\n",
    "    buf = f.read()\n",
    "    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "    num_images = len(data)/(image_size*image_size)\n",
    "    data = data.reshape(num_images, image_size, image_size, 1)\n",
    "    data = data/255.\n",
    "    return data\n",
    "\n",
    "def load_labels(filename):\n",
    "    f = gzip.open('train-labels-idx1-ubyte.gz','r')\n",
    "    f.read(8)\n",
    "    buf = f.read()\n",
    "    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    return labels\n",
    "\n",
    "#------------------------------------------------------#\n",
    "\n",
    "# Plot images\n",
    "def plot_images(data_element):\n",
    "    image = np.asarray(data_element).squeeze()\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "#------------------------------------------------------#\n",
    "\n",
    "# Model creation\n",
    "def create_cnn_model():\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 28, 28\n",
    "    num_classes = 10\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(15, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(10, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.SGD(),\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#------------------------------------------------------#\n",
    "\n",
    "# Model training\n",
    "def train_model(model, x_train, y_train):\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    print(num_classes)\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    epochs = 10\n",
    "    batch_size = 128\n",
    "    model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "### Main section starts here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading training data\n",
    "train_img = load_images('train-images-idx3-ubyte.gz')    \n",
    "train_labels = load_labels('train-labels-idx1-ubyte.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading test data\n",
    "test_img = load_images('train-images-idx3-ubyte.gz')    \n",
    "test_labels = load_labels('train-labels-idx1-ubyte.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4QeCIvxAUIQfCIrwA0H9H4BpmwJXvvG+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1167f6250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# visualizing data: plotting an image and the label.\n",
    "plot_images(train_img[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 15)        150       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 10)        1360      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5760)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                57610     \n",
      "=================================================================\n",
      "Total params: 59,120\n",
      "Trainable params: 59,120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initializing the model\n",
    "model = create_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 27s 446us/step - loss: 0.1421 - acc: 0.9600\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 23s 379us/step - loss: 0.1283 - acc: 0.9641\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 25s 421us/step - loss: 0.1179 - acc: 0.9672\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 27s 444us/step - loss: 0.1090 - acc: 0.9695\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 26s 440us/step - loss: 0.1016 - acc: 0.9714\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 26s 428us/step - loss: 0.0948 - acc: 0.9734\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 29s 477us/step - loss: 0.0902 - acc: 0.9744\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 31s 510us/step - loss: 0.0855 - acc: 0.9755\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 32s 530us/step - loss: 0.0809 - acc: 0.97672s  - ETA: 0s - loss: 0.0809 - acc: 0\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 32s 529us/step - loss: 0.0773 - acc: 0.9779\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_img, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.4975788e-08, 6.0749805e-09, 1.2134295e-05, 2.4021348e-01,\n",
       "        3.9778372e-12, 7.5972700e-01, 6.6278616e-08, 3.9552774e-06,\n",
       "        4.1382955e-05, 1.9488666e-06]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_img[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
